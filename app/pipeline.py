from sqlalchemy import select
from datetime import datetime
from dateutil import tz
import json, time

from .db import engine, SessionLocal, Base
from .models import News, Summary, Picks
from .config import DAILY_TOP_K, TIMEZONE
from .ingest import ingest_once
from .extract import extract_missing_text
from .dedupe import dedupe_titles
from .ranker import rank_recent
from .summarizer import summarize_article

def today_str():
    tzinfo = tz.gettz(TIMEZONE)
    return datetime.now(tz=tzinfo).strftime("%Y-%m-%d")

def run_pipeline():
    Base.metadata.create_all(bind=engine)

    # ğŸ“° 1. Crawl dá»¯ liá»‡u gá»‘c
    n_new = ingest_once()

    # ğŸ§  2. Extract ná»™i dung (náº¿u cÃ³ async)
    n_ext = 0
    try:
        n_ext = __import__("asyncio").run(
            __import__("app.extract", fromlist=["extract_missing_text"]).extract_missing_text()
        )
    except Exception:
        pass

    # ğŸ§© 3. Loáº¡i trÃ¹ng tiÃªu Ä‘á»
    n_dup = dedupe_titles()

    # ğŸ§® 4. Xáº¿p háº¡ng cÃ¡c bÃ i gáº§n Ä‘Ã¢y
    ranked = rank_recent(limit=120)

    # âš™ï¸ 5. Lá»c bÃ i cÃ³ ná»™i dung (text dÃ i > 200 kÃ½ tá»±)
    ranked_with_text = [r for _, r in ranked if r.content_text and len(r.content_text) > 200]

    # âœ… 6. Giá»›i háº¡n chá»‰ láº¥y 50 bÃ i Ä‘áº§u tiÃªn Ä‘á»ƒ tÃ³m táº¯t
    limited = ranked_with_text[:50]

    # ğŸ” 7. Sau Ä‘Ã³ chá»‰ chá»n top DAILY_TOP_K (máº·c Ä‘á»‹nh = 10)
    top = limited[:DAILY_TOP_K]

    with SessionLocal() as s:
        picks_date = today_str()
        # XoÃ¡ pick cÅ© trong DB
        s.execute(__import__("sqlalchemy").text("DELETE FROM picks WHERE date_str = :d"), {"d": picks_date})
        s.commit()

        created = 0
        for idx, news in enumerate(top, start=1):
            print(f"[{idx}/{len(top)}] ğŸ§¾ Summarizing: {news.title[:80]} ...")

            # ğŸª¶ 8. TÃ³m táº¯t báº±ng GPT hoáº·c offline fallback
            summ = summarize_article(news.url, news.title, news.source, news.content_text)

            # ğŸ•’ ThÃªm delay Ä‘á»ƒ trÃ¡nh rate limit (náº¿u cÃ³ key GPT)
            time.sleep(2)

            # ğŸ’¾ LÆ°u vÃ o DB
            summary = Summary(
                news_id=news.id,
                title_vi=summ.get("title_vi", news.title)[:250],
                bullets_json=json.dumps(summ.get("bullets", []), ensure_ascii=False),
                so_what_vn=summ.get("so_what_vn", ""),
                hashtags=",".join(summ.get("hashtags", []))[:120],
                attribution=summ.get("attribution", news.source),
                url=summ.get("url", news.url),
            )
            s.add(summary)
            s.flush()

            s.add(Picks(date_str=picks_date, rank=idx, news_id=news.id, summary_id=summary.id))
            created += 1

        s.commit()

    print(f"âœ… Pipeline done: {created} picks saved.")
    return {"new_items": n_new, "extracted": n_ext, "deduped": n_dup, "picks": created}

if __name__ == "__main__":
    res = run_pipeline()
    print(res)